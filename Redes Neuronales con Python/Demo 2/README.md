# Dise√±o, Entrenamiento y Optimizaci√≥n Integral de una Red Neuronal con Python

## üìö Descripci√≥n del Proyecto

Este proyecto implementa de forma **completa y unificada** todos los conceptos del **Cap√≠tulo 2: Configuraci√≥n y Ejecuci√≥n de Modelos Neuronales**. Es un proyecto integral que demuestra el ciclo completo de desarrollo de redes neuronales, desde el dise√±o de arquitecturas hasta la optimizaci√≥n y diagn√≥stico de rendimiento.

### üéØ Objetivos

- **2.1 Ingenier√≠a de Redes**: Dise√±ar y comparar 3 arquitecturas distintas
- **2.2 Protocolos de Entrenamiento**: Implementar entrenamiento con y sin controles
- **2.3 Optimizaci√≥n de Hiperpar√°metros**: Comparar Grid Search vs Random Search
- **2.4 Diagn√≥stico de Rendimiento**: Analizar underfitting y overfitting
- **2.5 Regularizaci√≥n**: Evaluar t√©cnicas de regularizaci√≥n (Dropout, L1, L2)

---

## üõ†Ô∏è Requisitos del Sistema

### Software Requerido

- **Sistema Operativo**: Windows 10/11
- **Python**: 3.10 o superior
- **Memoria RAM**: M√≠nimo 4GB (recomendado 8GB)

### Dependencias

```
torch>=2.0.0
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
```

---

## üì¶ Instalaci√≥n

### 1. Clonar o descargar el proyecto

```bash
cd "c:\Users\KevinDominich\1Python\Python\Personal\Portafolio\DeepLearningPython\Demo 2"
```

### 2. Instalar dependencias

```bash
python -m pip install -r neural_network_project/requirements.txt
```

### 3. Verificar instalaci√≥n

```bash
python -c "import torch; print(f'PyTorch {torch.__version__} instalado correctamente')"
```

---

## üöÄ C√≥mo Ejecutar el Proyecto

### Ejecuci√≥n Completa

Para ejecutar el proyecto completo (todos los componentes):

```bash
cd neural_network_project
python main.py
```

**‚è±Ô∏è Tiempo estimado**: 10-20 minutos (dependiendo del hardware)

### Ejecuci√≥n por Componentes

Si deseas ejecutar componentes individuales, puedes importar los m√≥dulos en un script de Python:

```python
from data.dataset_loader import get_data
from models.architectures import get_all_architectures
from training.trainer import NeuralNetworkTrainer

# Tu c√≥digo aqu√≠...
```

---

## üìÅ Estructura del Proyecto

```
neural_network_project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ dataset_loader.py          # Carga y preparaci√≥n de datos
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ architectures.py           # 3 arquitecturas de redes neuronales
‚îÇ   ‚îî‚îÄ‚îÄ model_utils.py             # Utilidades de regularizaci√≥n
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py                 # Protocolos de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ callbacks.py               # Early stopping, checkpoints
‚îÇ   ‚îî‚îÄ‚îÄ optimizer_search.py        # Grid/Random search
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ diagnostics.py             # Diagn√≥stico de rendimiento
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                 # C√°lculo de m√©tricas
‚îú‚îÄ‚îÄ visualization/
‚îÇ   ‚îî‚îÄ‚îÄ plots.py                   # Generaci√≥n de gr√°ficos
‚îú‚îÄ‚îÄ checkpoints/                   # Modelos guardados
‚îú‚îÄ‚îÄ results/                       # Gr√°ficos generados
‚îú‚îÄ‚îÄ main.py                        # Script principal
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias
‚îî‚îÄ‚îÄ README.md                      # Este archivo
```

---

## üß† Componentes del Proyecto

### 2.1 Ingenier√≠a de Redes

Se implementan **3 arquitecturas distintas**:

#### 1. **ShallowNet** (Red Superficial)
- **Capas**: 2 (1 hidden + 1 output)
- **Neuronas**: 64
- **Activaci√≥n**: ReLU
- **Learning Rate**: 0.01
- **Batch Size**: 32

#### 2. **DeepNet** (Red Profunda)
- **Capas**: 5 (4 hidden + 1 output)
- **Neuronas**: [128, 64, 32, 16]
- **Activaci√≥n**: ReLU
- **Learning Rate**: 0.001
- **Batch Size**: 64

#### 3. **RegularizedDeepNet** (Red Profunda Regularizada)
- **Capas**: 5 (4 hidden + 1 output)
- **Neuronas**: [128, 64, 32, 16]
- **Activaci√≥n**: ReLU
- **Dropout**: p=0.3
- **L2 Weight Decay**: 0.001
- **Learning Rate**: 0.001
- **Batch Size**: 64

**Visualizaciones**:
- Diagramas ASCII en consola
- Diagramas gr√°ficos con matplotlib (guardados en `results/`)

---

### 2.2 Protocolos de Entrenamiento

#### Divisi√≥n de Datos
- **Train**: 70% (con normalizaci√≥n)
- **Validation**: 15%
- **Test**: 15%

#### Protocolos Implementados

1. **Entrenamiento B√°sico** (sin controles)
   - Entrenamiento est√°ndar por √©pocas fijas
   - Sin early stopping ni checkpoints

2. **Entrenamiento con Callbacks**
   - **Early Stopping**: Detiene si validation loss no mejora (patience=10)
   - **Model Checkpoint**: Guarda mejor modelo basado en validation accuracy

3. **Reanudaci√≥n desde Checkpoint**
   - Carga modelo guardado
   - Contin√∫a entrenamiento desde √∫ltimo estado

**Visualizaciones**:
- Curvas de p√©rdida (train/val)
- Curvas de accuracy (train/val)

---

### 2.3 Optimizaci√≥n de Hiperpar√°metros

#### Espacio de B√∫squeda

```python
{
    'learning_rate': [0.001, 0.01, 0.1],
    'hidden_neurons': [32, 64, 128],
    'batch_size': [32, 64]
}
```

#### M√©todos Implementados

1. **Grid Search**
   - B√∫squeda exhaustiva de todas las combinaciones
   - Garantiza encontrar el √≥ptimo en el espacio definido
   - Mayor costo computacional

2. **Random Search**
   - Muestreo aleatorio de combinaciones
   - M√°s eficiente computacionalmente
   - Buena aproximaci√≥n al √≥ptimo

**Comparaci√≥n**:
- Tiempo de ejecuci√≥n
- Mejor accuracy alcanzada
- Eficiencia (accuracy/tiempo)

**Visualizaciones**:
- Gr√°ficos de barras comparativos
- Tabla de resultados

---

### 2.4 Diagn√≥stico de Rendimiento

#### An√°lisis de Underfitting

- **Modelo**: Red muy simple (1 capa, 8 neuronas)
- **S√≠ntomas**: Train y Val accuracy bajas (<70%)
- **Causa**: Modelo insuficiente para capturar patrones
- **Soluci√≥n**: Aumentar capacidad del modelo

#### An√°lisis de Overfitting

- **Modelo**: Red compleja sin regularizaci√≥n
- **S√≠ntomas**: Train accuracy >> Val accuracy (gap >10%)
- **Causa**: Memorizaci√≥n de datos de entrenamiento
- **Soluci√≥n**: Aplicar regularizaci√≥n

#### M√©tricas Calculadas

- **Accuracy**: Precisi√≥n general
- **Precision**: Precisi√≥n por clase
- **Recall**: Sensibilidad por clase
- **F1-Score**: Media arm√≥nica de precision y recall
- **Confusion Matrix**: Matriz de confusi√≥n

**Visualizaciones**:
- Curvas de aprendizaje comparativas
- Matrices de confusi√≥n
- Comparaci√≥n antes/despu√©s de mejoras

---

### 2.5 Regularizaci√≥n

#### T√©cnicas Implementadas

1. **Dropout** (p=0.3)
   - Desactiva aleatoriamente neuronas durante entrenamiento
   - Previene co-adaptaci√≥n de features

2. **L1 Regularization** (Œª=0.001)
   - Penaliza suma de valores absolutos de pesos
   - Promueve sparsity (pesos a cero)

3. **L2 Regularization** (Œª=0.001)
   - Penaliza suma de cuadrados de pesos
   - Promueve pesos peque√±os

#### Evaluaci√≥n

- Entrenamiento de modelo base (sin regularizaci√≥n)
- Entrenamiento con cada t√©cnica
- Comparaci√≥n de:
  - Validation accuracy
  - Gap train-val
  - Generalizaci√≥n

**Visualizaciones**:
- Gr√°ficos comparativos de curvas de aprendizaje
- Tabla de resultados

---

## üìä Resultados Esperados

Al ejecutar el proyecto, se generar√°n los siguientes archivos en `results/`:

### Arquitecturas
- `arch_shallow.png`: Diagrama de ShallowNet
- `arch_deep.png`: Diagrama de DeepNet
- `arch_regularized.png`: Diagrama de RegularizedDeepNet

### Entrenamiento
- `training_basic.png`: Curvas de entrenamiento b√°sico
- `training_callbacks.png`: Curvas con early stopping
- `training_resumed.png`: Curvas de entrenamiento reanudado

### Optimizaci√≥n
- `hyperparameter_search.png`: Comparaci√≥n Grid vs Random Search

### Diagn√≥stico
- `diagnostic_comparison.png`: Underfitting vs Overfitting
- `confusion_matrix_before.png`: Matriz antes de regularizaci√≥n
- `confusion_matrix_after.png`: Matriz despu√©s de regularizaci√≥n

### Regularizaci√≥n
- `regularization_comparison.png`: Comparaci√≥n de t√©cnicas

---

## üî¨ Decisiones de Ingenier√≠a

### Framework: PyTorch

**Justificaci√≥n**:
- Control granular sobre arquitecturas y entrenamiento
- Flexibilidad para implementar regularizaci√≥n personalizada (L1)
- Excelente documentaci√≥n y comunidad activa
- Mejor para prop√≥sitos educativos y experimentaci√≥n

### Dataset: Sint√©tico

**Justificaci√≥n**:
- Reproducibilidad garantizada (seed fija)
- Control sobre complejidad y ruido
- No requiere descarga externa
- Permite demostrar underfitting/overfitting de forma controlada

### Hiperpar√°metros

**Learning Rates**:
- ShallowNet: 0.01 (modelo simple, puede usar LR mayor)
- DeepNet: 0.001 (modelo profundo, requiere LR menor para estabilidad)

**Batch Sizes**:
- ShallowNet: 32 (suficiente para modelo simple)
- DeepNet: 64 (mayor batch para mejor estimaci√≥n de gradiente)

**Regularizaci√≥n**:
- Dropout: p=0.3 (est√°ndar, balance entre regularizaci√≥n y capacidad)
- L1/L2: Œª=0.001 (valor moderado, evita sobre-regularizaci√≥n)

---

## üêõ Soluci√≥n de Problemas

### Error: "No module named 'torch'"

**Soluci√≥n**:
```bash
python -m pip install torch
```

### Error: "CUDA not available"

**Soluci√≥n**: El proyecto funciona en CPU. Si deseas usar GPU, instala PyTorch con soporte CUDA desde [pytorch.org](https://pytorch.org)

### Gr√°ficos no se generan

**Soluci√≥n**: Verifica que matplotlib est√© instalado:
```bash
python -m pip install matplotlib seaborn
```

### Entrenamiento muy lento

**Soluci√≥n**: Reduce el n√∫mero de √©pocas o combinaciones en Grid Search editando `main.py`

---

## üìù Interpretaci√≥n de Resultados

### Curvas de Aprendizaje

- **Convergencia**: Train y Val loss deben decrecer
- **Overfitting**: Train loss << Val loss (divergencia)
- **Underfitting**: Ambas curvas altas y planas

### Matriz de Confusi√≥n

- **Diagonal**: Predicciones correctas
- **Fuera de diagonal**: Confusiones entre clases
- **Ideal**: Valores altos en diagonal, bajos fuera

### Comparaci√≥n de Regularizaci√≥n

- **Mejor t√©cnica**: Mayor Val Accuracy, menor gap train-val
- **Dropout**: Generalmente mejor para redes profundas
- **L2**: Bueno para prevenir pesos grandes
- **L1**: √ötil para feature selection

---

## üéì Conceptos Aprendidos

Este proyecto demuestra:

‚úÖ **Dise√±o de arquitecturas**: C√≥mo el n√∫mero de capas y neuronas afecta el rendimiento

‚úÖ **Protocolos de entrenamiento**: Importancia de early stopping y checkpoints

‚úÖ **Optimizaci√≥n**: Trade-off entre b√∫squeda exhaustiva y eficiencia

‚úÖ **Diagn√≥stico**: Identificaci√≥n y soluci√≥n de underfitting/overfitting

‚úÖ **Regularizaci√≥n**: T√©cnicas para mejorar generalizaci√≥n

---

## üìö Referencias

- **PyTorch Documentation**: https://pytorch.org/docs/
- **Scikit-learn**: https://scikit-learn.org/
- **Deep Learning Book**: Goodfellow, Bengio, Courville

---

## üë®‚Äçüíª Autor

Proyecto desarrollado como demostraci√≥n integral del Cap√≠tulo 2: Configuraci√≥n y Ejecuci√≥n de Modelos Neuronales.

---

## üìÑ Licencia

Este proyecto es de uso educativo.

---

## üéâ Conclusi√≥n

Este proyecto cubre **TODOS** los temas del Cap√≠tulo 2 de forma pr√°ctica y reproducible. Cada componente est√° implementado, documentado y justificado. Los resultados son visualizados y comparados para facilitar el aprendizaje.

**¬°Disfruta explorando el mundo de las redes neuronales!** üöÄ
